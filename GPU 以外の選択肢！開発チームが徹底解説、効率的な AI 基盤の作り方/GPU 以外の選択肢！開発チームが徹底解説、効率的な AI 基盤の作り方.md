### 1\. 開催概要

| 項目 | 内容 |
| :--- | :---------- |
| **セッション名** | GPU 以外の選択肢！開発チームが徹底解説、効率的な AI 基盤の作り方 |
| **日時** | 2025年6月25日 16:50 |
| **発表者** | 常世 大史 氏 (アマゾン ウェブ サービス ジャパン合同会社 Annapurna Labs, ML SA )|
| **概要** | AWSが自社開発するAIアクセラレータ「Trainium」および「Inferentia」について、その戦略と技術詳細、エコシステムを解説するセッション。AIモデルの大規模化に伴う計算コストの増大という課題に対し、GPU以外の選択肢として、これらカスタムシリコンがいかに低コスト・高性能・高電力効率なAI基盤を実現するかを紹介。最新チップ「Trainium2」を搭載したインスタンスや、世界最大規模の学習環境「Project Rainier」にも言及。セッション後半では、国内外の企業による活用事例を紹介するとともに、OSSの日本語LLMを用いて推論サーバーを構築する具体的な手順が解説された。 |

### 2\. 主要なポイント

#### AWSのカスタムシリコン戦略とAIアクセラレータ

AIモデルの進化に伴い、学習に必要な演算性能は指数関数的に増大している。 この状況に対応するため、AWSは性能・コスト・電力効率の最適化を目的とし、傘下のAnnapurna Labsを通じて半導体の自社開発に10年以上前から投資してきた。

その中核となるのが、AIワークロードに特化したアクセラレータ「AWS Trainium」と「AWS Inferentia」である。

  * **AWS Trainium**: 大規模モデルの「学習」に最適化されたアクセラレータ。
  * **AWS Inferentia**: モデルの「推論」処理を高性能かつ低コストで実行することに特化している。

両チップは「Neuronコア」と呼ばれる共通のプロセッサを搭載しており、用途に応じて最適化されている。

#### 最新チップ「Trainium2」の進化と大規模インフラ

2024年に一般提供が開始された第2世代の「Trainium2」は、性能が大幅に向上している。

  * **性能向上**: チップあたりのNeuronコアを8基に増やし、96GBの大容量HBMメモリを搭載。 これにより、同等のAmazon EC2インスタンスと比較して30〜40%高いコスト性能を実現する。
  * **電力効率**: チップと電圧レギュレータを近接配置する垂直方向の電力供給方式を採用し、初代Trainiumと比較して最大3倍の電力効率を達成した。
  * **新技術**: モデルの重みを圧縮して効率的に演算する「Structured Sparsity」技術に対応し、最大4倍のFLOPS性能を発揮する。

これらのチップを搭載したインフラも大規模化が進んでいる。

  * **Amazon EC2 Trn2 UltraServers**: 64個のTrainium2を広帯域・低遅延のNeuronLink-v3で接続した、機械学習向けサーバー。現在プレビュー中である。
  * **Project Rainier**: 数万個のAWS Trainium2を搭載し、単一の学習クラスターとしては最大のエクサFLOPS性能を実現するプロジェクト。Anthropic社が保有する既存の大規模学習クラスターと比較して5倍の計算能力を提供する。

#### エコシステムと導入事例

AWSは独自開発チップの使いやすさを高めるため、エコシステムの拡充にも注力している。

  * **AWS Neuron SDK**: Trainium/Inferentia上でモデルを効率的に実行するためのソフトウェア開発キット。PyTorchやJAXなどの主要な機械学習フレームワークをサポートし、モデルのコンパイルから実行までを担う。
  * **導入事例**:
      * **Amazon**: AIショッピングアシスタント「Rufus」や対話型AI「Alexa」といった自社サービスで活用。 2024年のプライムデーでは8万個以上のTrainium/Inferentiaを活用し、コストを4.5分の1に削減した。
      * **Anthropic社**: 最新モデル「Claude 3.5 Haiku」の推論にTrainium2を使用し、他のインスタンスと比較して60%の高速化を実現したと報告している。
      * **国内企業**: リコーがTrainiumを利用したLLM開発で50%のコスト削減を達成したほか、ELYZA社がInferentia2を用いて推論速度を約2倍に向上させるなど、国内でも活用が進んでいる。

#### 実践ガイド：推論サーバーの構築

セッションでは、vLLMとAmazon SageMakerを用いて推論サーバーを構築する2つの実践的な手順が紹介された。

1.  **vLLMを利用した構築**: EC2 Inf2インスタンス上で、オープンソースのライブラリ「vLLM」とNeuron SDKを使い、Llama-3.2 Visionモデルをデプロイする手順が示された。 この構成では、Llama3.2 11Bマルチモーダルモデルを1時間あたり0.76ドルという低コストで運用可能である。
2.  **Amazon SageMakerを利用した構築**: Hugging Face Hub上のモデルページから、Amazon SageMakerを選択し、表示されるスクリプトを実行するだけで、Qwen 2.5モデルの推論サーバーを簡単にデプロイできることが実演された。 Hugging Face上のコンパイル済みキャッシュを利用することで、デプロイ時間を短縮できる。

#### 今後の展望

AWSはカスタムシリコンへの投資を継続し、2025年内に次世代チップ「AWS Trainium3」の登場を予告した。 AWS初の3nmプロセスを採用し、Trainium2と比較して2倍の性能向上と40%の電力効率改善を目指す。 AWSは、こうした継続的なイノベーションを通じて、顧客のAI活用を支援していく姿勢を強調した。
