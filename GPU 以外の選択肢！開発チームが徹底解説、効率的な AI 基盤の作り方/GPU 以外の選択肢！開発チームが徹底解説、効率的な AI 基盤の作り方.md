<<<<<<< HEAD
### 1\. 開催概要

| 項目 | 内容 |
| :--- | :---------- |
| **セッション名** | GPU 以外の選択肢！開発チームが徹底解説、効率的な AI 基盤の作り方 |
| **日時** | 2025年6月25日 16:50 |
| **発表者** | 常世 大史 氏 (アマゾン ウェブ サービス ジャパン合同会社 Annapurna Labs, ML SA )|
| **概要** | AWSが自社開発するAIアクセラレータ「Trainium」および「Inferentia」について、その戦略と技術詳細、エコシステムを解説するセッション。AIモデルの大規模化に伴う計算コストの増大という課題に対し、GPU以外の選択肢として、これらカスタムシリコンがいかに低コスト・高性能・高電力効率なAI基盤を実現するかを紹介。最新チップ「Trainium2」を搭載したインスタンスや、世界最大規模の学習環境「Project Rainier」にも言及。セッション後半では、国内外の企業による活用事例を紹介するとともに、OSSの日本語LLMを用いて推論サーバーを構築する具体的な手順が解説された。 |

### 2\. 主要なポイント

#### AWSのカスタムシリコン戦略とAIアクセラレータ

AIモデルの進化に伴い、学習に必要な演算性能は指数関数的に増大している。 この状況に対応するため、AWSは性能・コスト・電力効率の最適化を目的とし、傘下のAnnapurna Labsを通じて半導体の自社開発に10年以上前から投資してきた。

その中核となるのが、AIワークロードに特化したアクセラレータ「AWS Trainium」と「AWS Inferentia」である。

  * **AWS Trainium**: 大規模モデルの「学習」に最適化されたアクセラレータ。
  * **AWS Inferentia**: モデルの「推論」処理を高性能かつ低コストで実行することに特化している。

両チップは「Neuronコア」と呼ばれる共通のプロセッサを搭載しており、用途に応じて最適化されている。

#### 最新チップ「Trainium2」の進化と大規模インフラ

2024年に一般提供が開始された第2世代の「Trainium2」は、性能が大幅に向上している。

  * **性能向上**: チップあたりのNeuronコアを8基に増やし、96GBの大容量HBMメモリを搭載。 これにより、同等のAmazon EC2インスタンスと比較して30〜40%高いコスト性能を実現する。
  * **電力効率**: チップと電圧レギュレータを近接配置する垂直方向の電力供給方式を採用し、初代Trainiumと比較して最大3倍の電力効率を達成した。
  * **新技術**: モデルの重みを圧縮して効率的に演算する「Structured Sparsity」技術に対応し、最大4倍のFLOPS性能を発揮する。

これらのチップを搭載したインフラも大規模化が進んでいる。

  * **Amazon EC2 Trn2 UltraServers**: 64個のTrainium2を広帯域・低遅延のNeuronLink-v3で接続した、機械学習向けサーバー。現在プレビュー中である。
  * **Project Rainier**: 数万個のAWS Trainium2を搭載し、単一の学習クラスターとしては最大のエクサFLOPS性能を実現するプロジェクト。Anthropic社が保有する既存の大規模学習クラスターと比較して5倍の計算能力を提供する。

#### エコシステムと導入事例

AWSは独自開発チップの使いやすさを高めるため、エコシステムの拡充にも注力している。

  * **AWS Neuron SDK**: Trainium/Inferentia上でモデルを効率的に実行するためのソフトウェア開発キット。PyTorchやJAXなどの主要な機械学習フレームワークをサポートし、モデルのコンパイルから実行までを担う。
  * **導入事例**:
      * **Amazon**: AIショッピングアシスタント「Rufus」や対話型AI「Alexa」といった自社サービスで活用。 2024年のプライムデーでは8万個以上のTrainium/Inferentiaを活用し、コストを4.5分の1に削減した。
      * **Anthropic社**: 最新モデル「Claude 3.5 Haiku」の推論にTrainium2を使用し、他のインスタンスと比較して60%の高速化を実現したと報告している。
      * **国内企業**: リコーがTrainiumを利用したLLM開発で50%のコスト削減を達成したほか、ELYZA社がInferentia2を用いて推論速度を約2倍に向上させるなど、国内でも活用が進んでいる。

#### 実践ガイド：推論サーバーの構築

セッションでは、vLLMとAmazon SageMakerを用いて推論サーバーを構築する2つの実践的な手順が紹介された。

1.  **vLLMを利用した構築**: EC2 Inf2インスタンス上で、オープンソースのライブラリ「vLLM」とNeuron SDKを使い、Llama-3.2 Visionモデルをデプロイする手順が示された。 この構成では、Llama3.2 11Bマルチモーダルモデルを1時間あたり0.76ドルという低コストで運用可能である。
2.  **Amazon SageMakerを利用した構築**: Hugging Face Hub上のモデルページから、Amazon SageMakerを選択し、表示されるスクリプトを実行するだけで、Qwen 2.5モデルの推論サーバーを簡単にデプロイできることが実演された。 Hugging Face上のコンパイル済みキャッシュを利用することで、デプロイ時間を短縮できる。

#### 今後の展望

AWSはカスタムシリコンへの投資を継続し、2025年内に次世代チップ「AWS Trainium3」の登場を予告した。 AWS初の3nmプロセスを採用し、Trainium2と比較して2倍の性能向上と40%の電力効率改善を目指す。 AWSは、こうした継続的なイノベーションを通じて、顧客のAI活用を支援していく姿勢を強調した。
=======
# GPU 以外の選択肢！開発チームが徹底解説、効率的な AI 基盤の作り方

## 日時
2025/6/25 16:50

## 発表者
常世 大史

アマゾン ウェブ サービス ジャパン合同会社
Annapurna Labs, ML SA

## abstract
AWS では低コストで高性能、電力効率の高い AI 基盤を実現するためのインフラ技術として、AI アクセラレータチップ AWS Trainium、Inferentia を自社開発してきました。本セッションでは、昨年末に一般公開開始した最新の Trainium2 搭載 Amazon EC2 Trn2 インスタンスから AWS Trn2 UltraServers、AWS 世界最大規模の学習環境となる Project Rainier まで、AWS のカスタムシリコン戦略の全容をご紹介します。セッション後半では、大規模言語モデル Claude を提供している Anthropic 社や、Alexa、Rufus といった Amazon の各種サービスなど、AWS Trainium/Inferentia を活用頂いているお客様の事例をご紹介するとともに、より実践的な内容として、日本語 OSS モデルを利用した推論サーバーの構築手順と TIPS について解説致します。本セッションは、大規模言語モデルの学習・推論環境の構築を検討されている方々、特にインフラコストの最適化に課題をお持ちの方に役立つ内容となっています。

## 概要メモ
- アンナプルナラボ
  - AWSが買収した半導体会社がもと
  - 半導体開発
  - 10年前買収
- コスト最適化には半導体開発が不可欠
- Trainium：GPU以外の機械学習推論？
- Inferentia2, Trainium：向きは違うが、内部では同じコアプロセッサ（Nuronコア）を利用している
- 1600Gpbsってすごいな……
- シストリックアレイ構造
  - 隣接する演算ユニットに渡していく
  - データの受け渡しは局所的に行われる→演算効率を最大限に高めている
- Inferentia2
  - 内部はTraniumとほぼ同じ
  - ネットワーク接続を簡素化→低コスト推論を実現
  - EFAは搭載していない
- Trainium2
  - 3200Gpbs!?
  - もう何がなんだかw
  - チップ内のコア数が2->8
  - Structured Spacity
    - 使用メモリを圧縮して演算することができる
- 電圧レギュレータ
  - 近ければ近いほどいい
- 機械学習のワークロードは即座に需要が発生する
- Tranium2を入れたサーバー
  - シンプルなデザインでケーブルが一歳ない
  - 空冷水冷どちらでも対応
- UltraServers
  - 名前、かっこいいなw
- 独自設計＝使い勝手が悪い？
- 国内外の事例紹介＆実際にサーバ構築
- Tranium活用事例
  - AlexaなどAmazonサービス
    - Rufus
  - Anthropic
    - Claude 3.5 haikuはTranium2を使用している
- サーバの構築手順
- Nuron SDK
- NxD：PyTorch上で分散推論向けのライブラリ
- NeuronSDK
  - EKSなどAWS各種サービス＋サードパーティ製と綿密に連携可能
- PyTorch上でNxDを使用する環境を選択
- SageMaker
  - 推論：SageMaker Studio
- HuggingFace TGI：HuggingFace上のモデルを簡単にデプロイできる
  - 表示されたスクリプトをコピーして実行するだけ！
  - コンパイル済みのキャッシュがあれば、デプロイ時間を短縮可能
- 
>>>>>>> c4022de53fd400287e7b5dcadd1ad68adbe05274
