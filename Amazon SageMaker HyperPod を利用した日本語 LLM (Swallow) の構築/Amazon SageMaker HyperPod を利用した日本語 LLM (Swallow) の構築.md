### 1. 開催概要
| 項目 | 内容 |
| :--- | :---------- |
| **セッション名** | Amazon SageMaker HyperPod を利用した日本語 LLM (Swallow) の構築 |
| **日時** | 2025/06/25 12:40~ |
| **発表者** | 藤井 一喜 氏（東京科学大学 情報理工学院 修士課程 2 年） |
| **概要** | 東京科学大学と産業技術総合研究所が共同で開発する日本語特化LLMシリーズ「Swallow」について、その最新版モデルの性能向上（特に日本語能力、コード生成、数学的推論能力）と、モデル構築・学習にAmazon SageMaker HyperPodを活用した際の知見が紹介された。AWSのサービス構成、分散学習環境のセットアップ、大規模学習のベストプラクティスに加え、Swallow Projectの今後の取り組みについても言及された。 |

### 2. 主要なポイント

* **Swallow Projectの概要**:
    * オープンモデルを活用し、日本語に強い大規模言語モデル (LLM) を研究開発するプロジェクトである。
    * 東京科学大学（NLP）と産業技術総合研究所（HPC、ML）の共同研究として進められている。

* **継続事前学習 (Continual Pre-Training)**:
    * 既存の英語基盤モデル（Llama-3, Gemma-2など）をファインチューニングする手法を採用しており、独自に収集した日本語、英語、コードのデータを使用している。
    * 利点として、オープンLLMの活用と比較的低コストでの学習が可能であることが挙げられるが、元モデルのアーキテクチャやライセンスに制約があり、英語スコアの低下や破壊的忘却といった課題もある。

* **推論能力の向上**:
    * 数学やコードといった推論能力が問われるタスクにおいて、AIの性能が急速に向上していることが示された（特にo3, DeepSeek-R1など）。
    * しかし、単に数学やコードデータを明示的に追加するだけでは性能が上がりにくいという課題も存在した。

* **Swallowモデルの性能**:
    * **Llama-3.3-Swallow-70B-v0.4**:
        * 2025年3月10日にリリースされ、日本語QA能力、コード能力が強化された。
        * 日本語理解・生成タスク（学術タスク）において、GPT-4oに匹敵するか、Qwen-2.5 72Bを上回る性能を示した。
        * しかし、英語、数学、コードの分野では依然として改善の余地がある。
    * **Llama-3.1-Swallow-8B-v0.5**:
        * 2025年6月にリリース予定で、日本語QA能力、コード能力、数学能力がさらに強化された。
        * コード生成能力は、独自のコードコーパス「SwallowCode」を利用した4段階の高品質パイプラインにより、HumanEvalで+19.1ポイント、JHumanEvalで+15.5ポイントの大幅な改善を達成した。
        * 数学能力も、高品質な「SwallowMath」コーパスの利用により、MGSMで+10.8、GSM8Kで+17.8、MATHで+16.2ポイントの大幅な改善を実現した。

* **学習データの構成**:
    * Llama-3.3-Swallow-70B-v0.4の学習データは合計315Bトークンであり、そのうち日本語が約60%を占め、残りを英語、数学、コードのデータが構成する。
    * 特に日本語QA能力強化のため、教育的価値の高いテキストを選別するためにLLMベースの分類器が利用され、Wikipedia的なものだけでなく、高品質な合成テキストが採用された。

* **学習の高速化**:
    * Megatron-LMを採用し、DP communication Overlap、TP communication Overlap、Async checkpoint (dist checkpoint) といった高速化手法を用いている。
    * 通信 (communication) と計算 (computation) のOverlapにより、モデル性能を低下させることなく高速化を実現している。ただし、実装は複雑になる。

* **AWSの活用とインフラストラクチャ**:
    * **Amazon SageMaker HyperPod**: NVIDIA H100 GPUを搭載したP5.48xlarge 32インスタンスで学習を実施。これまではスパコンでの学習が主であったが、AWSへの移行の容易さやジョブの最適化機能が評価された。
    * **監視基盤**: Amazon Managed Service for PrometheusとAmazon Managed Grafanaを用いて、学習時に発生する障害情報収集とエラー発生時の問題究明を迅速化し、ダウンタイムの最小化を実現している。
    * **ストレージ**: Amazon FSx for LustreとAmazon S3を連携させ、データ転送の簡素化と高速化を図っている。特にData Repository Association (DRA) を利用することで、学習準備と計算ノードのデプロイに集中でき、コスト効率も向上させている。

* **Swallow Projectの今後**:
    * **モデルの高性能化**: 数学・コード能力のさらなる強化、金融・医療・法律といった専門ドメイン知識の強化を目指す。
    * **チューニングモデル**: 強化学習によるReasoning能力の強化、thinkモードとchatモードの動的な切り替えを検討する。
    * **学習・推論の低コスト化**: FP8やBlockwise Quantizationといった低精度学習の実用化や、SSM、Hybridモデルなどのモデルアーキテクチャの変更を検討する。
    * スパコンが古いGPUを搭載している場合があるため、最新のGPUを利用するためにAWSを積極的に活用していく方針である。
    * 開発されたコードと数学コーパスはHugging Faceで公開される予定である（SwallowCode, SwallowMath）。
