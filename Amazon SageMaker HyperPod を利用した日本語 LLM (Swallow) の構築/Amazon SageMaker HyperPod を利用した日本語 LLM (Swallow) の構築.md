<<<<<<< HEAD
### 1. 開催概要
| 項目 | 内容 |
| :--- | :---------- |
| **セッション名** | Amazon SageMaker HyperPod を利用した日本語 LLM (Swallow) の構築 |
| **日時** | 2025/06/25 12:40~ |
| **発表者** | 藤井 一喜 氏（東京科学大学 情報理工学院 修士課程 2 年） |
| **概要** | 東京科学大学と産業技術総合研究所が共同で開発する日本語特化LLMシリーズ「Swallow」について、その最新版モデルの性能向上（特に日本語能力、コード生成、数学的推論能力）と、モデル構築・学習にAmazon SageMaker HyperPodを活用した際の知見が紹介された。AWSのサービス構成、分散学習環境のセットアップ、大規模学習のベストプラクティスに加え、Swallow Projectの今後の取り組みについても言及された。 |

### 2. 主要なポイント

* **Swallow Projectの概要**:
    * オープンモデルを活用し、日本語に強い大規模言語モデル (LLM) を研究開発するプロジェクトである。
    * 東京科学大学（NLP）と産業技術総合研究所（HPC、ML）の共同研究として進められている。

* **継続事前学習 (Continual Pre-Training)**:
    * 既存の英語基盤モデル（Llama-3, Gemma-2など）をファインチューニングする手法を採用しており、独自に収集した日本語、英語、コードのデータを使用している。
    * 利点として、オープンLLMの活用と比較的低コストでの学習が可能であることが挙げられるが、元モデルのアーキテクチャやライセンスに制約があり、英語スコアの低下や破壊的忘却といった課題もある。

* **推論能力の向上**:
    * 数学やコードといった推論能力が問われるタスクにおいて、AIの性能が急速に向上していることが示された（特にo3, DeepSeek-R1など）。
    * しかし、単に数学やコードデータを明示的に追加するだけでは性能が上がりにくいという課題も存在した。

* **Swallowモデルの性能**:
    * **Llama-3.3-Swallow-70B-v0.4**:
        * 2025年3月10日にリリースされ、日本語QA能力、コード能力が強化された。
        * 日本語理解・生成タスク（学術タスク）において、GPT-4oに匹敵するか、Qwen-2.5 72Bを上回る性能を示した。
        * しかし、英語、数学、コードの分野では依然として改善の余地がある。
    * **Llama-3.1-Swallow-8B-v0.5**:
        * 2025年6月にリリース予定で、日本語QA能力、コード能力、数学能力がさらに強化された。
        * コード生成能力は、独自のコードコーパス「SwallowCode」を利用した4段階の高品質パイプラインにより、HumanEvalで+19.1ポイント、JHumanEvalで+15.5ポイントの大幅な改善を達成した。
        * 数学能力も、高品質な「SwallowMath」コーパスの利用により、MGSMで+10.8、GSM8Kで+17.8、MATHで+16.2ポイントの大幅な改善を実現した。

* **学習データの構成**:
    * Llama-3.3-Swallow-70B-v0.4の学習データは合計315Bトークンであり、そのうち日本語が約60%を占め、残りを英語、数学、コードのデータが構成する。
    * 特に日本語QA能力強化のため、教育的価値の高いテキストを選別するためにLLMベースの分類器が利用され、Wikipedia的なものだけでなく、高品質な合成テキストが採用された。

* **学習の高速化**:
    * Megatron-LMを採用し、DP communication Overlap、TP communication Overlap、Async checkpoint (dist checkpoint) といった高速化手法を用いている。
    * 通信 (communication) と計算 (computation) のOverlapにより、モデル性能を低下させることなく高速化を実現している。ただし、実装は複雑になる。

* **AWSの活用とインフラストラクチャ**:
    * **Amazon SageMaker HyperPod**: NVIDIA H100 GPUを搭載したP5.48xlarge 32インスタンスで学習を実施。これまではスパコンでの学習が主であったが、AWSへの移行の容易さやジョブの最適化機能が評価された。
    * **監視基盤**: Amazon Managed Service for PrometheusとAmazon Managed Grafanaを用いて、学習時に発生する障害情報収集とエラー発生時の問題究明を迅速化し、ダウンタイムの最小化を実現している。
    * **ストレージ**: Amazon FSx for LustreとAmazon S3を連携させ、データ転送の簡素化と高速化を図っている。特にData Repository Association (DRA) を利用することで、学習準備と計算ノードのデプロイに集中でき、コスト効率も向上させている。

* **Swallow Projectの今後**:
    * **モデルの高性能化**: 数学・コード能力のさらなる強化、金融・医療・法律といった専門ドメイン知識の強化を目指す。
    * **チューニングモデル**: 強化学習によるReasoning能力の強化、thinkモードとchatモードの動的な切り替えを検討する。
    * **学習・推論の低コスト化**: FP8やBlockwise Quantizationといった低精度学習の実用化や、SSM、Hybridモデルなどのモデルアーキテクチャの変更を検討する。
    * スパコンが古いGPUを搭載している場合があるため、最新のGPUを利用するためにAWSを積極的に活用していく方針である。
    * 開発されたコードと数学コーパスはHugging Faceで公開される予定である（SwallowCode, SwallowMath）。
=======
# Amazon SageMaker HyperPod を利用した日本語 LLM (Swallow) の構築

## 日時
2025/06/25 12:40~

## 発表者
藤井 一喜 氏

東京科学大学 情報理工学院
修士課程 2 年

## abstract
東京科学大学と産業技術総合研究所の研究チームが開発する LLM シリーズ Swallow は、2023 年以降、多数の日本語特化モデルをリリースしてきました。2025 年 3 月に公開した最新版モデルでは、日本語能力のいっそうの強化に加え、コード生成や数学的推論能力を高めることに成功し、高い性能を示しています。本講演では、本モデルを Amazon SageMaker HyperPod を用いて構築・学習した際に得られた知見を中心に、活用した AWS のサービス構成・分散学習環境のセットアップや、実際に行った大規模学習のベストプラクティスを紹介します。また、Swallow Project が現在取り組んでいる試みについても一部紹介します。

## 概要メモ
- Swallow Project
  - オープンモデルをしようして日本語に強いLLMを研究開発する
  - 東京科学大学＆産総研の共同研究
    - 岡崎：NLP
    - 横田：HPC、ML
- 継続次元学習
  - 英語の基盤モデルをファインチューニング
  - データは独自に収集
  - 欠点
    - モデルの構造に制約がある
- データを入れれば性能が上がるわけじゃない
  - 明示的なデータを入れても、数学的な能力が上がっていない
- 今のAIは数学的思考能力が高いモデルが流行り
- Llama -3.1-Swallow -8B ~v05
  - 数学能力、コード能力強化
- 70B-v0.4
  - 4o > Swallow > Alibaba
- 日本語QA能力の強化
  - 教育的価値の高いテキストを厳選
  - 分類器をもとに厳選
  - Wikipedia的なものではなく
- コード生成能力
  - 独自のコードコーパスをしよう
- 数学能力 
  - 独自のコーパスを作成
- コーパスはコード、数学、両方HuggingFaceで公開
- 学習データの比率
  - 日本語60%
  - 英語、多少なりとも
  - 数学とコードをある程度
  - 315Bトークン
- 高速化手法を用いている
- Communication Overlap
  - Computation Communication Overlap
  - All Gatherとフォワードを同時に行う
  - モデルの性能を低下させずに高速化
  - 実装の複雑化
- P5インスタンス
  - Amazon SageMaker HyperPod
  - これまではスパコンでやっていたが、AWSではインスタンスを個別で立てる必要
  - 移行がめんどくさい
  - 設定ファイルを書く必要があるが、移行がそこまでめんどくさくない
  - ジョブを最適化してくれる
  - Amazon Managed Grafanaで監視ツールを用いてエラー発生時の問題究明を迅速化
  - ストレージ
    - FSx for LustreとS3間でデータ転送を高速化
    - モデルのスナップショットを撮る時間を短縮
    - FSx：高コスト
    - DRA： S3にデータを上げておいて、学習が始まる直前にFSxを立ち上げてほぼ自動的にデータを転送＆低コスト化
- 今後
  - モデルの高性能か
    - 数学とコード能力の強化
    - 専門性が高い分野での知識強化
  - Reasoning能力
    - 強化学習
  - chatとthinkの動的な切り替え
  - 低精度学習
    - 8bitの精度でより高速に推論
  - モデルのアーキテクチャ
    - Hybridモデル
- Swallow ProjectとAWS
  - スパコンで難しい時にはAWSをしようする
  - スパコンは古いGPUが入っている場合が多い
    - 最新のGPUをAWS利用
>>>>>>> c4022de53fd400287e7b5dcadd1ad68adbe05274
