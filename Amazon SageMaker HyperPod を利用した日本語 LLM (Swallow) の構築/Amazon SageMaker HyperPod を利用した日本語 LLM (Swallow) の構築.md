# Amazon SageMaker HyperPod を利用した日本語 LLM (Swallow) の構築

## 日時
2025/06/25 12:40~

## 発表者
藤井 一喜 氏

東京科学大学 情報理工学院
修士課程 2 年

## abstract
東京科学大学と産業技術総合研究所の研究チームが開発する LLM シリーズ Swallow は、2023 年以降、多数の日本語特化モデルをリリースしてきました。2025 年 3 月に公開した最新版モデルでは、日本語能力のいっそうの強化に加え、コード生成や数学的推論能力を高めることに成功し、高い性能を示しています。本講演では、本モデルを Amazon SageMaker HyperPod を用いて構築・学習した際に得られた知見を中心に、活用した AWS のサービス構成・分散学習環境のセットアップや、実際に行った大規模学習のベストプラクティスを紹介します。また、Swallow Project が現在取り組んでいる試みについても一部紹介します。

## 概要メモ
- Swallow Project
  - オープンモデルをしようして日本語に強いLLMを研究開発する
  - 東京科学大学＆産総研の共同研究
    - 岡崎：NLP
    - 横田：HPC、ML
- 継続次元学習
  - 英語の基盤モデルをファインチューニング
  - データは独自に収集
  - 欠点
    - モデルの構造に制約がある
- データを入れれば性能が上がるわけじゃない
  - 明示的なデータを入れても、数学的な能力が上がっていない
- 今のAIは数学的思考能力が高いモデルが流行り
- Llama -3.1-Swallow -8B ~v05
  - 数学能力、コード能力強化
- 70B-v0.4
  - 4o > Swallow > Alibaba
- 日本語QA能力の強化
  - 教育的価値の高いテキストを厳選
  - 分類器をもとに厳選
  - Wikipedia的なものではなく
- コード生成能力
  - 独自のコードコーパスをしよう
- 数学能力 
  - 独自のコーパスを作成
- コーパスはコード、数学、両方HuggingFaceで公開
- 学習データの比率
  - 日本語60%
  - 英語、多少なりとも
  - 数学とコードをある程度
  - 315Bトークン
- 高速化手法を用いている
- Communication Overlap
  - Computation Communication Overlap
  - All Gatherとフォワードを同時に行う
  - モデルの性能を低下させずに高速化
  - 実装の複雑化
- P5インスタンス
  - Amazon SageMaker HyperPod
  - これまではスパコンでやっていたが、AWSではインスタンスを個別で立てる必要
  - 移行がめんどくさい
  - 設定ファイルを書く必要があるが、移行がそこまでめんどくさくない
  - ジョブを最適化してくれる
  - Amazon Managed Grafanaで監視ツールを用いてエラー発生時の問題究明を迅速化
  - ストレージ
    - FSx for LustreとS3間でデータ転送を高速化
    - モデルのスナップショットを撮る時間を短縮
    - FSx：高コスト
    - DRA： S3にデータを上げておいて、学習が始まる直前にFSxを立ち上げてほぼ自動的にデータを転送＆低コスト化
- 今後
  - モデルの高性能か
    - 数学とコード能力の強化
    - 専門性が高い分野での知識強化
  - Reasoning能力
    - 強化学習
  - chatとthinkの動的な切り替え
  - 低精度学習
    - 8bitの精度でより高速に推論
  - モデルのアーキテクチャ
    - Hybridモデル
- Swallow ProjectとAWS
  - スパコンで難しい時にはAWSをしようする
  - スパコンは古いGPUが入っている場合が多い
    - 最新のGPUをAWS利用
